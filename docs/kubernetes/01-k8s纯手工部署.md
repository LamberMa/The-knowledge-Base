# Kubernetes二进制部署

> - 文档日期：2018年12月25日
>
> - 记录人员：lamber
>
> 环境说明：master节点\*3，node节点\*2，三台master节点做负载均衡，通过阿里云的内网SLB做负载，根据需要，node节点可以动态的进行添加。当前基本结构如下图：

![](http://tuku.dcgamer.top/18-12-25/86810633.jpg)

## IP地址分配

| Name                 | IP                   | Description   |
| -------------------- | -------------------- | ------------- |
| Kubernetes Master HA | 172.16.16.124（VIP） | master HA地址 |
| Pro-k8s-master1      | 172.16.16.119        | master-1      |
| Pro-k8s-master2      | 172.16.16.120        | master-2      |
| Pro-k8s-master3      | 172.16.16.121        | master-3      |
| Pro-k8s-node1        | 172.16.16.122        | node-1        |
| Pro-k8s-node2        | 172.16.16.123        | node-2        |
| ETCD_NODE1           | 172.16.16.119        | etcd cluster  |
| ETCD_NODE2           | 172.16.16.120        | etcd cluster  |
| ETCD_NODE3           | 172.16.16.121        | etcd cluster  |
|                      |                      |               |

## 系统初始化

- 安装部署Docker`指定版本为18.06.0-ce`，并修改Docker的数据存放位置为`/alidata/docker/`。（略）

- 准备Kubernetes的1.12.3的相关二进制文件，具体可参考[CHANGELOG-1.12](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.12.md#v1123)。（略）

- 系统主机名初始化，主机SSH授信免密登录。（略）

- 关闭selinux，关闭iptables（firewalld）。

- 准备部署目录，并把/alidata/kubernetes/bin加入到环境变量PATH中。

  ```shell
   mkdir -p /alidata/kubernetes/{cfg,bin,ssl,log}
  ```

## 手动制作CA证书

> 本次生成证书的工具采用CFSSL，CFSSL是CloudFlare开源的一款PKI/TLS工具。 CFSSL 包含一个命令行工具和一个用于签名的工具，验证并且捆绑TLS证书的 HTTP API 服务。 使用Go语言编写。我们可以使用json去定义证书相关内容，看起来更加直观。
>
> CFSSL包括：
>
> - 一组用于生成自定义 TLS PKI 的工具
> - `cfssl`程序，是CFSSL的命令行工具
> - `multirootca`程序是可以使用多个签名密钥的证书颁发机构服务器
> - `mkbundle`程序用于构建证书池
> - `cfssljson`程序，从`cfssl`和`multirootca`程序获取JSON输出，并将证书，密钥，CSR和bundle写入磁盘

### 准备cfssl的二进制文件

```shell
# 下载cfssl二进制文件
wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64
wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64
wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64
chmod +x cfssl*
mv cfssl-certinfo_linux-amd64 /alidata/kubernetes/bin/cfssl-certinfo
mv cfssljson_linux-amd64  /alidata/kubernetes/bin/cfssljson
mv cfssl_linux-amd64  /alidata/kubernetes/bin/cfssl

# 分发到所有的k8s节点，因为设置了免密因此无需输入密码，其余几个节点的分发省略，只写一个。
scp /alidata/kubernetes/bin/cfssl* root@172.16.16.120:/alidata/kubernetes/bin/
```

### 初始化cfssl

```shell
mkdir /alidata/src && cd /alidata/src
cfssl print-defaults config > config.json
cfssl print-defaults csr > csr.json
```

### 创建用来生成CA文件的JSON配置文件

```json
# 证书有效期约为10年
[root@Pro-k8s-master1 /alidata/src/ssl]# cat ca-config.json 
{
     "signing": {
       "default": {
          "expiry": "87600h"
           },
       "profiles": {
          "kubernetes": {
            "usages": [
              "signing",
              "key encipherment",
              "server auth",
              "client auth"
             ],
           "expiry": "87600h"
            }
         }
     }
}
```

### 创建用来生成CA证书签名请求（CSR）的JSON配置文件

```json
[root@Pro-k8s-master1 /alidata/src/ssl]# cat ca-csr.json 
{
  "CN": "kubernetes",
  "key": {
    "algo": "rsa",
    "size": 2048
   },
  "names": [
   {
     "C": "CN",
     "ST": "BeiJing",
     "L": "BeiJing",
     "O": "k8s",
     "OU": "System"
    }
  ]
}
```

### 生成CA证书（ca.pem）和密钥（ca-key.pem）

```shell
cfssl gencert -initca ca-csr.json | cfssljson -bare ca
```

### 分发证书

```shell
cp ca.csr ca.pem ca-key.pem ca-config.json /alidata/kubernetes/ssl

# 分发到其他的节点
scp -P 5203 ca.csr ca.pem ca-key.pem ca-config.json 172.16.16.120:/alidata/kubernetes/ssl 
```

## ETCD集群部署

> 本次etcd集群主要部署在三个master节点，同时为了方便扩展因此在制作证书的时候就把集群所有的ip都做进去，然后分发实现证书可以通用。

#### 准备ETCD的二进制软件

```shell
# etcd版本为3.2.18，
wget https://github.com/coreos/etcd/releases/download/v3.2.18/etcd-v3.2.18-linux-amd64.tar.gz
tar zxf etcd-v3.2.18-linux-amd64.tar.gz
cd etcd-v3.2.18-linux-amd64

# 分发软件
chmod +x etcd*
cp etcd etcdctl /alidata/kubernetes/bin/ 
scp -P 5203 etcd etcdctl 172.16.16.120:/alidata/kubernetes/bin/
scp -P 5203 etcd etcdctl 172.16.16.121:/alidata/kubernetes/bin/
```

### 创建ETCD证书签名请求

```shell
[root@Pro-k8s-master1 /alidata/src/ssl]# cat etcd-csr.json 
{
  "CN": "etcd",
  "hosts": [
    "127.0.0.1",
    "172.16.16.119",
    "172.16.16.120",
    "172.16.16.121",
    "172.16.16.122",
    "172.16.16.123",
    "172.16.16.124"
    ],
  "key": {
    "algo": "rsa",
    "size": 2048
    },
  "names": [
    {
     "C": "CN",
     "ST": "BeiJing",
     "L": "BeiJing",
     "O": "k8s",
     "OU": "System"
     }
  ]
}
```

#### 生成证书和私钥

```shell
cfssl gencert -ca=/alidata/kubernetes/ssl/ca.pem \
  -ca-key=/alidata/kubernetes/ssl/ca-key.pem \
  -config=/alidata/kubernetes/ssl/ca-config.json \
  -profile=kubernetes etcd-csr.json | cfssljson -bare etcd
```

### 分发证书和私钥

```shell
cp etcd*.pem /alidata/kubernetes/ssl
scp -P 5203 etcd*.pem 172.16.16.120:/alidata/kubernetes/ssl
scp -P 5203 etcd*.pem 172.16.16.121:/alidata/kubernetes/ssl
```

### 设置ETCD配置文件

etcd集群中的每一个节点都要有，记得要改etcd的name，以及监听的地址。

```shell
[root@Pro-k8s-master1 /alidata/src/ssl]# cat /alidata/kubernetes/cfg/etcd.conf
#[member]
ETCD_NAME="etcd-node1"
ETCD_DATA_DIR="/var/lib/etcd/default.etcd"
#ETCD_SNAPSHOT_COUNTER="10000"
#ETCD_HEARTBEAT_INTERVAL="100"
#ETCD_ELECTION_TIMEOUT="1000"
ETCD_LISTEN_PEER_URLS="https://172.16.16.119:2380"
ETCD_LISTEN_CLIENT_URLS="https://172.16.16.119:2379,https://127.0.0.1:2379"
#ETCD_MAX_SNAPSHOTS="5"
#ETCD_MAX_WALS="5"
#ETCD_CORS=""
#[cluster]
ETCD_INITIAL_ADVERTISE_PEER_URLS="https://172.16.16.119:2380"
# if you use different ETCD_NAME (e.g. test),
# set ETCD_INITIAL_CLUSTER value for this name, i.e. "test=http://..."
ETCD_INITIAL_CLUSTER="etcd-node1=https://172.16.16.119:2380,etcd-node2=https://172.16.16.120:2380,etcd-node3=https://172.16.16.121:2380"
ETCD_INITIAL_CLUSTER_STATE="new"
ETCD_INITIAL_CLUSTER_TOKEN="k8s-etcd-cluster"
ETCD_ADVERTISE_CLIENT_URLS="https://172.16.16.119:2379"
#[security]
CLIENT_CERT_AUTH="true"
ETCD_CA_FILE="/alidata/kubernetes/ssl/ca.pem"
ETCD_CERT_FILE="/alidata/kubernetes/ssl/etcd.pem"
ETCD_KEY_FILE="/alidata/kubernetes/ssl/etcd-key.pem"
PEER_CLIENT_CERT_AUTH="true"
ETCD_PEER_CA_FILE="/alidata/kubernetes/ssl/ca.pem"
ETCD_PEER_CERT_FILE="/alidata/kubernetes/ssl/etcd.pem"
ETCD_PEER_KEY_FILE="/alidata/kubernetes/ssl/etcd-key.pem"
```

### 创建ETCD的系统服务

```shell
[root@Pro-k8s-master1 /alidata/src/ssl]# cat /etc/systemd/system/etcd.service    
[Unit]
Description=Etcd Server
After=network.target

[Service]
Type=simple
WorkingDirectory=/var/lib/etcd
EnvironmentFile=-/alidata/kubernetes/cfg/etcd.conf
# set GOMAXPROCS to number of processors
ExecStart=/bin/bash -c "GOMAXPROCS=$(nproc) /alidata/kubernetes/bin/etcd"
Type=notify

[Install]
WantedBy=multi-user.target
```

### 重载系统服务

```shell
systemctl daemon-reload
systemctl enable etcd

# 分发配置文件，分发后在另外两台机器重载配置文件
scp -P 5203 /etc/systemd/system/etcd.service 172.16.16.120:/etc/systemd/system/
scp -P 5203 /etc/systemd/system/etcd.service 172.16.16.121:/etc/systemd/system/

# 同步启动所有的etcd节点
mkdir /var/lib/etcd
systemctl start etcd
systemctl status etcd
```

### 验证集群

```shell
[root@Pro-k8s-master1 ~]# etcdctl --endpoints=https://172.16.16.119:2379 \
   --ca-file=/alidata/kubernetes/ssl/ca.pem \
   --cert-file=/alidata/kubernetes/ssl/etcd.pem \
   --key-file=/alidata/kubernetes/ssl/etcd-key.pem cluster-health
member 140dd484abeb416a is healthy: got healthy result from https://172.16.16.119:2379
member 97de764ba7fa74f2 is healthy: got healthy result from https://172.16.16.121:2379
member f2ab1be129de49b0 is healthy: got healthy result from https://172.16.16.120:2379
cluster is healthy
```

## K8S Master 节点部署

### 准备软件包

准备下载好的1.12.3版本的软件包，解压后将kube-apiserver，kube-controller-manager，kube-scheduler拷贝到所有的master节点的`/alidata/kubernetes/bin`目录下。

### 创建生成CSR的JSON配置文件

这里我除了把k8s集群中的所有节点都加进去了，以及kubernetes的ip 10.1.0.1和阿里云的私网负载均衡的地址我也加进去了，避免后续controller和scheduler访问异常。

```shell
[root@Pro-k8s-master1 /alidata/src/ssl]# cat kubernetes-csr.json 
{
  "CN": "kubernetes",
  "hosts": [
    "127.0.0.1",
    "172.16.16.119",
    "172.16.16.120",
    "172.16.16.121",
    "172.16.16.122",
    "172.16.16.123",
    "172.16.16.124",
    "10.1.0.1",
    "kubernetes",
    "kubernetes.default",
    "kubernetes.default.svc",
    "kubernetes.default.svc.cluster",
    "kubernetes.default.svc.cluster.local"
    ],
  "key": {
    "algo": "rsa",
    "size": 2048
   },
  "names": [
    {
     "C": "CN",
     "ST": "BeiJing",
     "L": "BeiJing",
     "O": "k8s",
     "OU": "System"
    }
  ]
}
```

### 生成Kubernetes的证书和私钥

```shell
cfssl gencert -ca=/alidata/kubernetes/ssl/ca.pem \
   -ca-key=/alidata/kubernetes/ssl/ca-key.pem \
   -config=/alidata/kubernetes/ssl/ca-config.json \
   -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetes

# 分发证书
cp kubernetes*.pem /alidata/kubernetes/ssl/
# 注意分发到所有节点，其他节点略
scp -P 5203 kubernetes*.pem 172.16.16.120:/alidata/kubernetes/ssl/ 
```

### 创建kube-apiserver使用的客户端token文件

```shell
[root@Pro-k8s-master1 /alidata/src/ssl]# head -c 16 /dev/urandom | od -An -t x | tr -d ' '
ad6d5bb607a186796d8861557df0d17f

# master节点分发
[root@Pro-k8s-master1 /alidata/src/ssl]# cat /alidata/kubernetes/ssl/bootstrap-token.csv
ad6d5bb607a186796d8861557df0d17f,kubelet-bootstrap,10001,"system:kubelet-bootstrap"

scp -P 5203 /alidata/kubernetes/ssl/bootstrap-token.csv 172.16.16.120:/alidata/kubernetes/ssl/
scp -P 5203 /alidata/kubernetes/ssl/bootstrap-token.csv 172.16.16.121:/alidata/kubernetes/ssl/
```

### 创建基础用户名/密码认证配置

```shell
[root@Pro-k8s-master1 /alidata/src/ssl]# cat /alidata/kubernetes/ssl/basic-auth.csv 
admin,admin,1
readonly,readonly,2

# master节点分发
scp -P 5203 /alidata/kubernetes/ssl/basic-auth.csv 172.16.16.120:/alidata/kubernetes/ssl/
scp -P 5203 /alidata/kubernetes/ssl/basic-auth.csv 172.16.16.121:/alidata/kubernetes/ssl/
```

### 配置kube-apiserver的系统配置

```shell
[root@Pro-k8s-master1 /alidata/src/ssl]# cat /usr/lib/systemd/system/kube-apiserver.service 
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target

[Service]
ExecStart=/alidata/kubernetes/bin/kube-apiserver \
  --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota,NodeRestriction \
  --bind-address=0.0.0.0 \
  --insecure-bind-address=0.0.0.0 \
  --authorization-mode=Node,RBAC \
  --runtime-config=rbac.authorization.k8s.io/v1 \
  --kubelet-https=true \
  --anonymous-auth=false \
  --basic-auth-file=/alidata/kubernetes/ssl/basic-auth.csv \
  --enable-bootstrap-token-auth \
  --token-auth-file=/alidata/kubernetes/ssl/bootstrap-token.csv \
  --service-cluster-ip-range=10.1.0.0/16 \
  --service-node-port-range=20000-40000 \
  --tls-cert-file=/alidata/kubernetes/ssl/kubernetes.pem \
  --tls-private-key-file=/alidata/kubernetes/ssl/kubernetes-key.pem \
  --client-ca-file=/alidata/kubernetes/ssl/ca.pem \
  --service-account-key-file=/alidata/kubernetes/ssl/ca-key.pem \
  --etcd-cafile=/alidata/kubernetes/ssl/ca.pem \
  --etcd-certfile=/alidata/kubernetes/ssl/kubernetes.pem \
  --etcd-keyfile=/alidata/kubernetes/ssl/kubernetes-key.pem \
  --etcd-servers=https://172.16.16.119:2379,https://172.16.16.120:2379,https://172.16.16.121:2379 \
  --enable-swagger-ui=true \
  --allow-privileged=true \
  --audit-log-maxage=30 \
  --audit-log-maxbackup=3 \
  --audit-log-maxsize=100 \
  --audit-log-path=/alidata/kubernetes/log/api-audit.log \
  --event-ttl=1h \
  --v=2 \
  --logtostderr=false \
  --log-dir=/alidata/kubernetes/log
Restart=on-failure
RestartSec=5
Type=notify
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
```

### 启动api server

```shell
systemctl daemon-reload
systemctl enable kube-apiserver
systemctl start kube-apiserver
systemctl status kube-apiserver
```

### 为API Server做负载

因为使用的是阿里云的ECS，因此直接购买了一个阿里云的内网SLB实现负载分发的功能为apiserver做ha，注意购买时选择的网络类型要和现有的机器是一致的，否则无法添加后端节点。

![](http://tuku.dcgamer.top/18-12-7/97104972.jpg)

在选择转发的时候用于提供https的6443端口转发到后端的8080，http的8080转发到后端的8080，注意https这里要添加证书内容。

![](http://tuku.dcgamer.top/18-12-7/82554621.jpg)

图内证书内容为阿里云提供的证书样例内容，需要将我们自己生成的kubernetes.pem和kubernetes-key.pem的内容手动填写到这里，并且将证书应用才可以。

#### 准备controller-manager的服务配置文件

注意这里指定的master已经是SLB的IP了，配置完成以后将配置文件分发到所有的master

```shell
# 在高可用状态下要开启leader-elect=true
[root@Pro-k8s-master1 /alidata/src/ssl]# cat /usr/lib/systemd/system/kube-controller-manager.service
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
ExecStart=/alidata/kubernetes/bin/kube-controller-manager \
  --address=0.0.0.0 \
  --master=http://172.16.16.124:8080 \
  --allocate-node-cidrs=true \
  --service-cluster-ip-range=10.1.0.0/16 \
  --cluster-cidr=10.2.0.0/16 \
  --cluster-name=kubernetes \
  --cluster-signing-cert-file=/alidata/kubernetes/ssl/ca.pem \
  --cluster-signing-key-file=/alidata/kubernetes/ssl/ca-key.pem \
  --service-account-private-key-file=/alidata/kubernetes/ssl/ca-key.pem \
  --root-ca-file=/alidata/kubernetes/ssl/ca.pem \
  --leader-elect=true \
  --v=2 \
  --logtostderr=false \
  --log-dir=/alidata/kubernetes/log

Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
```

### 启动Controller-Manager

```shell
systemctl daemon-reload
systemctl enable kube-controller-manager
systemctl start kube-controller-manager
systemctl status kube-controller-manager
```

### 准备kube-scheduler的服务配置文件

这里指定的master同样是SLB的VIP地址，配置完成以后分发到所有的master节点。

```shell
[root@Pro-k8s-master1 /alidata/src/ssl]# cat /usr/lib/systemd/system/kube-scheduler.service 
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
ExecStart=/alidata/kubernetes/bin/kube-scheduler \
  --address=0.0.0.0 \
  --master=http://172.16.16.124:8080 \
  --leader-elect=true \
  --v=2 \
  --logtostderr=false \
  --log-dir=/alidata/kubernetes/log

Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
```

### 启动kube-scheduler

```shell
systemctl daemon-reload
systemctl enable kube-scheduler
systemctl start kube-scheduler
systemctl status kube-scheduler
```

### 部署kubectl

首先准备好kubectl的二进制文件分发到所有的master节点，然后创建admin的证书签名请求

```shell
[root@Pro-k8s-master1 /alidata/src/ssl]# cat admin-csr.json 
{
  "CN": "admin",
  "hosts": [],
  "key": {
    "algo": "rsa",
    "size": 2048
   },
  "names": [
     {
       "C": "CN",
       "ST": "BeiJing",
       "L": "BeiJing",
       "O": "system:masters",
       "OU": "System"
      }
  ]
}
```

生成admin证书和私钥

```shell
cfssl gencert -ca=/alidata/kubernetes/ssl/ca.pem \
   -ca-key=/alidata/kubernetes/ssl/ca-key.pem \
   -config=/alidata/kubernetes/ssl/ca-config.json \
   -profile=kubernetes admin-csr.json | cfssljson -bare admin
```

设置集群参数，注意master的ip为vip，证书以嵌入的形式生成config文件。

```shell
kubectl config set-cluster kubernetes \
   --certificate-authority=/alidata/kubernetes/ssl/ca.pem \
   --embed-certs=true \
   --server=https://172.16.16.124:6443
```

设置客户端认证参数

```shell
kubectl config set-credentials admin \
   --client-certificate=/alidata/kubernetes/ssl/admin.pem \
   --embed-certs=true \
   --client-key=/alidata/kubernetes/ssl/admin-key.pem
```

设置上下文参数

```shell
kubectl config set-context kubernetes \
   --cluster=kubernetes \
   --user=admin
```

设置默认上下文

```shell
kubectl config use-context kubernetes
```

查看当前config

```shell
# 查看的这个文件在/root/.kube目录下，名称为config，上述操作需要在所有master节点操作
[root@Pro-k8s-master1 /alidata/src/ssl]# kubectl config view
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: DATA+OMITTED
    server: https://172.16.16.124:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: admin
  name: kubernetes
current-context: kubernetes
kind: Config
preferences: {}
users:
- name: admin
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED
```

使用kubectl工具

```shell
[root@Pro-k8s-master1 /alidata/src/ssl]# kubectl get cs
NAME                 STATUS    MESSAGE              ERROR
controller-manager   Healthy   ok                   
scheduler            Healthy   ok                   
etcd-1               Healthy   {"health": "true"}   
etcd-0               Healthy   {"health": "true"}   
etcd-2               Healthy   {"health": "true"} 
```

## Node节点部署

准备node节点二进制包，将kubelet，kube-proxy分发到所有node节点的/alidata/kubernetes/bin下。

创建角色绑定

```shell
kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --user=kubelet-bootstrap
```

创建 kubelet bootstrapping kubeconfig 文件 设置集群参数

```shell
kubectl config set-cluster kubernetes \
   --certificate-authority=/alidata/kubernetes/ssl/ca.pem \
   --embed-certs=true \
   --server=https://172.16.16.124:6443 \
   --kubeconfig=bootstrap.kubeconfig
```

设置客户端认证参数

```shell
# 注意这token是kube-apiserver使用的客户端token文件中的token，填写在这里。
kubectl config set-credentials kubelet-bootstrap \
   --token=ad6d5bb607a186796d8861557df0d17f  \
   --kubeconfig=bootstrap.kubeconfig 
```

设置上下文参数

```shell
kubectl config set-context default \
   --cluster=kubernetes \
   --user=kubelet-bootstrap \
   --kubeconfig=bootstrap.kubeconfig
```

选择默认上下文

```shell
kubectl config use-context default --kubeconfig=bootstrap.kubeconfig
```

因为指定了`--kubeconfig`，因此会在当前路径下生成一个bootstarp.kubeconfig文件，我们需要将这个kubeconfig文件分发到其他节点，因为目前只有master上有kubectl命令，因此在master上操作玩了以后把这个文件分发就可以

```shell
cp bootstrap.kubeconfig /alidata/kubernetes/cfg

# 只有node节点需要
scp -P 5203 bootstrap.kubeconfig 172.16.16.122:/alidata/kubernetes/cfg
scp -P 5203 bootstrap.kubeconfig 172.16.16.123:/alidata/kubernetes/cfg
```

设置CNI的支持

```shell
mkdir -p /etc/cni/net.d
[root@Pro-k8s-node1 ~]# vim /etc/cni/net.d/10-default.conf
{
  "name": "flannel",
  "type": "flannel",
  "delegate": {
    "bridge": "docker0",
    "isDefaultGateway": true,
    "mtu": 1400
    }
}
```

创建kubelet目录

```shell
mkdir /var/lib/kubelet
```

准备kubelet的服务配置文件

```shell
[root@Pro-k8s-node1 ~]# cat /usr/lib/systemd/system/kubelet.service
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=docker.service
Requires=docker.service

[Service]
WorkingDirectory=/var/lib/kubelet
ExecStart=/alidata/kubernetes/bin/kubelet \
  --address=172.16.16.122 \
  --hostname-override=172.16.16.122 \
  --pod-infra-container-image=mirrorgooglecontainers/pause-amd64:3.0 \
  --experimental-bootstrap-kubeconfig=/alidata/kubernetes/cfg/bootstrap.kubeconfig \
  --kubeconfig=/alidata/kubernetes/cfg/kubelet.kubeconfig \
  --cert-dir=/alidata/kubernetes/ssl \
  --network-plugin=cni \
  --cni-conf-dir=/etc/cni/net.d \
  --cni-bin-dir=/alidata/kubernetes/bin/cni \
  --cluster-dns=10.1.0.2 \
  --cluster-domain=cluster.local. \
  --hairpin-mode hairpin-veth \
  --allow-privileged=true \
  --fail-swap-on=false \
  --logtostderr=true \
  --v=2 \
  --logtostderr=false \
  --log-dir=/alidata/kubernetes/log
Restart=on-failure
RestartSec=5
```

启动kubelet

```shell
systemctl daemon-reload
systemctl enable kubelet
systemctl start kubelet
systemctl status kubelet
```

查看这个时候kubelet是否启动正常，如果node节点的kubelet启动是正常的话那么就可以在master节点查看到请求了，回到master上查看。

```shell
# master节点
kubectl get csr
```

默认看到的CONDITION内容应该是`Pending`状态的，此时需要批准kubelet的TLS认证请求。

```shell
kubectl get csr|grep 'Pending' | awk 'NR>0{print $1}'| xargs kubectl certificate approve
```

执行完毕后，查看节点状态已经是Ready的状态了

```shell
[root@Pro-k8s-master1 /alidata/src/ssl]# kubectl get node
NAME            STATUS   ROLES    AGE    VERSION
172.16.16.122   Ready    <none>   5h1m   v1.12.3
172.16.16.123   Ready    <none>   5h1m   v1.12.3
```

### 部署kube-proxy

> kube-proxy同样还是只部署在node节点上

在之前的操作中，已经将kube-proxy的二进制文件分发到node节点了，因此这里开始准备证书的json文件。

```json
[root@Pro-k8s-master1 /alidata/src/ssl]# cat kube-proxy-csr.json 
{
  "CN": "system:kube-proxy",
  "hosts": [],
  "key": {
    "algo": "rsa",
    "size": 2048
    },
  "names": [
     {
       "C": "CN",
       "ST": "BeiJing",
       "L": "BeiJing",
       "O": "k8s",
       "OU": "System"
     }
  ]
}
```

生成证书

```shell
cfssl gencert -ca=/alidata/kubernetes/ssl/ca.pem \
   -ca-key=/alidata/kubernetes/ssl/ca-key.pem \
   -config=/alidata/kubernetes/ssl/ca-config.json \
   -profile=kubernetes  kube-proxy-csr.json | cfssljson -bare kube-proxy
```

分发证书到所有的(Node)节点

```shell
cp kube-proxy*.pem /alidata/kubernetes/ssl/
scp -P 5203 kube-proxy*.pem 172.16.16.122:/alidata/kubernetes/ssl/
scp -P 5203 kube-proxy*.pem 172.16.16.123:/alidata/kubernetes/ssl/
```

创建kube-proxy的kubeconfig配置文件

```shell
kubectl config set-cluster kubernetes \
   --certificate-authority=/alidata/kubernetes/ssl/ca.pem \
   --embed-certs=true \
   --server=https://172.16.16.124:6443 \
   --kubeconfig=kube-proxy.kubeconfig
   
   
kubectl config set-credentials kube-proxy \
   --client-certificate=/alidata/kubernetes/ssl/kube-proxy.pem \
   --client-key=/alidata/kubernetes/ssl/kube-proxy-key.pem \
   --embed-certs=true \
   --kubeconfig=kube-proxy.kubeconfig
   
kubectl config set-context default \
   --cluster=kubernetes \
   --user=kube-proxy \
   --kubeconfig=kube-proxy.kubeconfig
   
kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig
```

和kubelet一样，把这个kube-config文件分发到所有的node节点

```shell
cp kube-proxy.kubeconfig /alidata/kubernetes/cfg/
scp -P 5203 kube-proxy.kubeconfig 172.16.16.122:/alidata/kubernetes/cfg/
scp -P 5203 kube-proxy.kubeconfig 172.16.16.123:/alidata/kubernetes/cfg/
```

创建kubeproxy的服务配置文件

```shell
mkdir /var/lib/kube-proxy

[root@Pro-k8s-node2 ~]# cat /usr/lib/systemd/system/kube-proxy.service
[Unit]
Description=Kubernetes Kube-Proxy Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target

[Service]
WorkingDirectory=/var/lib/kube-proxy
ExecStart=/alidata/kubernetes/bin/kube-proxy \
  --bind-address=172.16.16.123 \
  --hostname-override=172.16.16.123 \
  --kubeconfig=/alidata/kubernetes/cfg/kube-proxy.kubeconfig \
  --masquerade-all \
  --feature-gates=SupportIPVSProxyMode=true \
  --proxy-mode=ipvs \
  --ipvs-min-sync-period=5s \
  --ipvs-sync-period=5s \
  --ipvs-scheduler=rr \
  --logtostderr=true \
  --v=2 \
  --logtostderr=false \
  --log-dir=/alidata/kubernetes/log
            
Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
```

安装kube-proxy使用lvs的依赖。

```shell
yum install -y ipvsadm ipset conntrack
```

启动kube-proxy

```shell
systemctl daemon-reload
systemctl enable kube-proxy
systemctl start kube-proxy
systemctl status kube-proxy
```

在node节点检查kube-proxy的服务状态

```shell
ipvsadm -L -n
```

## Flannel部署

> 考虑到阿里云的网络暂时并不支持BGP，因此无法使用calico-BGP，而calico-ipip和flannel-vxlan在性能上相差无几，但是calico的维护难度相较于flannel要高很多，因此这里采用flannel作为网络组件。

创建flannel的证书json文件

```shell
[root@Pro-k8s-master1 /alidata/src/ssl]# cat flanneld-csr.json 
{
  "CN": "flanneld",
  "hosts": [],
  "key": {
    "algo": "rsa",
    "size": 2048
    },
  "names": [
    {
      "C": "CN",
      "ST": "BeiJing",
      "L": "BeiJing",
      "O": "k8s",
      "OU": "System"
     }
  ]
}
```

生成证书并分发证书

```shell
cfssl gencert -ca=/alidata/kubernetes/ssl/ca.pem \
   -ca-key=/alidata/kubernetes/ssl/ca-key.pem \
   -config=/alidata/kubernetes/ssl/ca-config.json \
   -profile=kubernetes flanneld-csr.json | cfssljson -bare flanneld
   
# 注意这个是分发到集群里的每一个节点
cp flanneld*.pem /alidata/kubernetes/ssl/
scp -P 5203 flanneld*.pem 172.16.16.120:/alidata/kubernetes/ssl/
```

准备flannel的二进制文件

```shell
cp flanneld mk-docker-opts.sh remove-docker0.sh /alidata/kubernetes/bin/

# 分发到其他的节点，此处只列出一个。
scp -P 5203 flanneld mk-docker-opts.sh remove-docker0.sh 172.16.16.120:/alidata/kubernetes/bin/
```

配置flannel，配置完成以后进行分发，保证每一个节点都有一个这个配置文件供flannel使用。

```shell
[root@Pro-k8s-master1 /alidata/src/ssl]# cat /alidata/kubernetes/cfg/flannel
FLANNEL_ETCD="-etcd-endpoints=https://172.16.16.119:2379,https://172.16.16.120:2379,https://172.16.16.121:2379"
FLANNEL_ETCD_KEY="-etcd-prefix=/kubernetes/network"
FLANNEL_ETCD_CAFILE="--etcd-cafile=/alidata/kubernetes/ssl/ca.pem"
FLANNEL_ETCD_CERTFILE="--etcd-certfile=/alidata/kubernetes/ssl/flanneld.pem"
FLANNEL_ETCD_KEYFILE="--etcd-keyfile=/alidata/kubernetes/ssl/flanneld-key.pem"
```

设置flannel的系统服务配置

```shell
[root@Pro-k8s-master1 /alidata/src/ssl]# cat /usr/lib/systemd/system/flannel.service
[Unit]
Description=Flanneld overlay address etcd agent
After=network.target
Before=docker.service

[Service]
EnvironmentFile=-/alidata/kubernetes/cfg/flannel
ExecStartPre=/alidata/kubernetes/bin/remove-docker0.sh
ExecStart=/alidata/kubernetes/bin/flanneld ${FLANNEL_ETCD} ${FLANNEL_ETCD_KEY} ${FLANNEL_ETCD_CAFILE} ${FLANNEL_ETCD_CERTFILE} ${FLANNEL_ETCD_KEYFILE}
ExecStartPost=/alidata/kubernetes/bin/mk-docker-opts.sh -d /run/flannel/docker

Type=notify

[Install]
WantedBy=multi-user.target
RequiredBy=docker.service
```

flannel集成CNI

```shell
# https://github.com/containernetworking/plugins/releases
wget https://github.com/containernetworking/plugins/releases/download/v0.7.1/cni-plugins-amd64-v0.7.1.tgz

# 同样将cni组件分发到每一个节点上一份
mkdir /alidata/kubernetes/bin/cni
tar zxf cni-plugins-amd64-v0.7.1.tgz -C /alidata/kubernetes/bin/cni
chmod +x /alidata/kubernetes/bin/*
scp -P 5203 -r /alidata/kubernetes/bin/cni/* 172.16.16.120:/alidata/kubernetes/bin/cni/
```

设置etcd的key

```shell
/alidata/kubernetes/bin/etcdctl --ca-file /alidata/kubernetes/ssl/ca.pem --cert-file /alidata/kubernetes/ssl/flanneld.pem --key-file /alidata/kubernetes/ssl/flanneld-key.pem \
      --no-sync -C https://172.16.16.119:2379,https://172.16.16.120:2379,https://172.16.16.121:2379 \
mk /kubernetes/network/config '{ "Network": "10.2.0.0/16", "Backend": { "Type": "vxlan", "VNI": 1 }}' >/dev/null 2>&1
```

查看这个key是否设置成功，只在一个节点操作就可以，因为这个etcd是一个集群。

```shell
[root@Pro-k8s-master1 /alidata/src/ssl]# etcdctl --endpoints=https://172.16.16.119:2379 \
   --ca-file=/alidata/kubernetes/ssl/ca.pem \
   --cert-file=/alidata/kubernetes/ssl/etcd.pem \
   --key-file=/alidata/kubernetes/ssl/etcd-key.pem get /kubernetes/network/config
{ "Network": "10.2.0.0/16", "Backend": { "Type": "vxlan", "VNI": 1 }}
```

配置Docker使用flannel

```shell
[root@Pro-k8s-master1 /alidata/src/ssl]# cat /etc/systemd/system/docker.service
[Unit]
Description=Docker Application Container Engine
Documentation=http://docs.docker.io
After=network-online.target firewalld.service flannel.service
Wants=network-online.target
Requires=flannel.service

[Service]
Environment="PATH=/usr/local/docker/bin:/bin:/sbin:/usr/bin:/usr/sbin"
EnvironmentFile=-/run/flannel/docker
#ExecStart=/usr/local/docker/bin/dockerd --log-level=error $DOCKER_NETWORK_OPTIONS
ExecStart=/usr/local/docker/bin/dockerd $DOCKER_OPTS
ExecReload=/bin/kill -s HUP $MAINPID
Restart=on-failure
RestartSec=5
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity
Delegate=yes
KillMode=process

[Install]
WantedBy=multi-user.target
```

主要有几个点，一个是After添加flannel.server，Requires要添加flannel.service，如果网络起不来的话，那么docker就不起来。同时在Service段添加EnvironmentFile=-/run/flannel/docker，这个文件是由flannel启动的时候通过mk-docker-opts.sh生成的，可以看一下这个文件的内容，通过—bip来划分网段。

```shell
[root@Pro-k8s-master1 /alidata/src/ssl]# cat /run/flannel/docker 
DOCKER_OPT_BIP="--bip=10.2.26.1/24"
DOCKER_OPT_IPMASQ="--ip-masq=true"
DOCKER_OPT_MTU="--mtu=1450"
DOCKER_OPTS=" --bip=10.2.26.1/24 --ip-masq=true --mtu=1450"
```

重启docker

```shell
systemctl daemon-reload
systemctl restart docker
```

观察网卡信息

```shell
[root@Pro-k8s-master1 /alidata/src/ssl]# ifconfig
docker0: flags=4099<UP,BROADCAST,MULTICAST>  mtu 1500
        inet 10.2.26.1  netmask 255.255.255.0  broadcast 10.2.26.255
        ether 02:42:97:b9:64:bd  txqueuelen 0  (Ethernet)
        RX packets 0  bytes 0 (0.0 B)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 0  bytes 0 (0.0 B)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

eth0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
        inet 172.16.16.119  netmask 255.255.255.0  broadcast 172.16.16.255
        ether 00:16:3e:08:1b:46  txqueuelen 1000  (Ethernet)
        RX packets 8365198  bytes 1280511623 (1.1 GiB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 8723511  bytes 1741235760 (1.6 GiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

flannel.1: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1450
        inet 10.2.26.0  netmask 255.255.255.255  broadcast 0.0.0.0
        ether 92:d2:90:73:57:5c  txqueuelen 0  (Ethernet)
        RX packets 2678  bytes 1052685 (1.0 MiB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 2100  bytes 297679 (290.7 KiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

lo: flags=73<UP,LOOPBACK,RUNNING>  mtu 65536
        inet 127.0.0.1  netmask 255.0.0.0
        loop  txqueuelen 1  (Local Loopback)
        RX packets 3848161  bytes 647614198 (617.6 MiB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 3848161  bytes 647614198 (617.6 MiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
```

可以看到docker0和flannel1.1的地址是一段的，每个节点分到的网段是不一样的。flannel部署完成

## 一些必备组件

### CoreDns

略

### Helm

略

### DashBoard

略